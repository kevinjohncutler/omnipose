{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18db5131-dc98-4636-bf5e-829858248e1e",
   "metadata": {},
   "source": [
    "```{eval-rst}\n",
    ".. include:: sinebow.rst\n",
    "\n",
    "```\n",
    "\n",
    ":sinebow20:`N-color`\n",
    "=====================\n",
    "\n",
    "Here I will argue that many of the errors I see in ground-truth datasets can be most kindly attributed to a lack of good label visualization. To illustrate, here is a moderately dense field of view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc478ab-f92a-48c8-9908-2f83fd474037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make local editable packages automatically reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "import numpy as np\n",
    "from omnipose.utils import rescale, crop_bbox\n",
    "import fastremap\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "from cellpose import io, plot\n",
    "\n",
    "basedir = os.path.join(Path.cwd(),'test_files') #run the mono_channel_bact notebook to generate masks\n",
    "masks = io.imread(os.path.join(basedir,'masks','ec_5I_t141xy5c1_cp_masks.tif'))\n",
    "img = io.imread(os.path.join(basedir,'ec_5I_t141xy5c1.tif'))\n",
    "plt.imshow(plot.outline_view(img,masks))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f7c977-09a2-488a-8a4c-0dc7a9743530",
   "metadata": {},
   "source": [
    "This outline view clearly distinguishes cells from each other, and it requires just one color (or channel). As ground truth, binary maps like this are one of the easiest annotations to generate and are therefore quite common in public datasets (see MiSiC, DeLTA, and SuperSegger just for a few in the realm of bacterial microscopy). However, this mode of annotation does not guarantee that shared boundaries between two cells are **precisely** 2px thick. Without this, the resulting label matrix will either exclude boundary pixels or asymmetrically incorporate them into one of the two cells. This is a primary reason why label matrices, not boundary maps, must be the primary / fundamental ground truth used to train and evaluate any segmentation algorithm. \n",
    "\n",
    "However, creating and editing label matrices has its own set of issues. If you have too many cells in an image, you quickly run out of distinct colors to distinguish adjacent cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec48e28b-30b6-4b89-b6e7-95bc0712733f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "bbx = crop_bbox(masks) #in omni\n",
    "slc = bbx[0]\n",
    "m,_ = fastremap.renumber(masks[slc]) # make sure masks go from 0 to N\n",
    "print('number of masks: ', np.max(m))\n",
    "\n",
    "cmap = mpl.cm.get_cmap('viridis')\n",
    "pic = cmap(rescale(m))\n",
    "pic[:,:,-1] = m>0 # alpha \n",
    "plt.imshow(pic)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39252f3-a69e-4fa8-82bc-e5688f70b980",
   "metadata": {},
   "source": [
    "This perceptually uniform color map is our best bet of distinguishing cells from each other, but some close cells are too similar to tell apart. The standard technique is to randomly shuffle the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb869d46-fbef-4200-b1b0-5e1e460867b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = fastremap.unique(m)\n",
    "vals = keys.copy()\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(keys)\n",
    "d = dict(zip(keys,vals))\n",
    "m_shuffle = fastremap.remap(m,d)\n",
    "pic = cmap(rescale(m_shuffle))\n",
    "pic[:,:,-1] = m>0 # alpha \n",
    "plt.imshow(pic)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d6daba-8e7a-41dc-b405-9de9033099b0",
   "metadata": {},
   "source": [
    "This doesn't fix the problem. You might think that adding more colors would help..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd3ae78-f8c1-4a4b-ae51-fb03e6a4b138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omnipose.utils import sinebow\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "cmap = ListedColormap([color for color in list(sinebow(m.max()).values())[1:]])\n",
    "pic = cmap(m_shuffle)\n",
    "pic[:,:,-1] = m>0 # alpha \n",
    "plt.imshow(pic)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed2b79f-e6c4-47a6-933d-daef779538e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "... but since even random shuffling *does not guarantee* that numerically close labels become spatially separated, adjacent labels that were hard to tell apart using a perceptually uniform color map like viridis are often *more difficult* to tell apart using any kind of unicorn-vomit color map. \n",
    "\n",
    "Worse still, multiple similar colors can accidentally get used while editing the *wrong cell* (*e.g.*, color 11 inside cell 12 that are both shades of yellow) and ruin the segmentation despite this error being imperceptible \n",
    "to the human eye (this may account for many of the \"errant pixels\" we observe across ground-truth datasets of dense cells). \n",
    "\n",
    "To solve this problem, I developed the `ncolor`_ package, which converts $K$-integer label matrices to $N \\ll K$ - color labels. The `four color theorem`_ \n",
    "guarantees that you only need 4 unique cell labels to cover all cells, but my algorithm opts to use 5 if a solution using 4 is not found quickly.\n",
    "This was integral in developing the BPCIS dataset, and I subsequently incorporated it into Cellpose and Omnipose. By default, the GUI and plot commands display N-color \n",
    "masks for easier visualization and editing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cc370c-2f05-4b27-bf6a-0d30673c8867",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ncolor \n",
    "cmap = mpl.cm.get_cmap('viridis')\n",
    "pic = cmap(rescale(ncolor.label(m)))\n",
    "pic[:,:,-1] = m>0 # alpha \n",
    "plt.imshow(pic)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181af187-1afb-4760-8d65-cd39dceda5b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "Interesting note: my code works for 3D volume labels as well, but there is no analogous theorem guaranteeing any sort of upper bound $N<K$ in 3D. \n",
    "In 3D, you could in principle have cells that touch every other cell, in which case $N=K$ and you cannot \"recolor your map\". On the dense but otherwise \n",
    "well-behaved volumes I have tested, my algorithm ends up needing 6-7 unique labels. I am curious if some bound on N can be formulated in the context of constrained volumes,\n",
    "*e.g.*, packed spheres of mixed and arbitrary diameter...\n",
    "\n",
    "Final note: thanks to Ryan Peters for suggesting a fix for displaying segmentations that (a) are from ground-truth sets with pixel-separated (boundary-map-generated) label matrices or (b) have many sparse, disjoint objects. By expanding labels before coloring them (a step that actually takes far longer than the coloring step itself), we get a much more pleasing distribution of colors that can make it easier to assess segmentations when when images are zoomed out. For example, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185787c7-13e3-41b4-9573-2bbae17f0fb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "masks = io.imread(os.path.join(basedir,'masks','caulo_15_cp_masks.tif'))\n",
    "exp = ncolor.expand_labels(masks)\n",
    "pic = cmap(rescale(np.hstack([ncolor.label(masks,expand=False),ncolor.label(exp),ncolor.label(masks)])))\n",
    "pic[:,:,-1] = np.hstack([masks,exp,masks])>0 # alpha \n",
    "plt.imshow(pic)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c66995-5e84-40af-a2d6-def9834db729",
   "metadata": {},
   "source": [
    "(Left: ncolor applied to raw masks. Middle: ncolor expanded masks. Right: current version of ncolor). Note that the expansion itself takes about 2x longer than the ncolor algorithm itself takes to run, but the extra milliseconds are worth it. If you know of any faster way to get a feature transform than `scipy.ndimage`, please let me know. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bd87de-8d7d-4d9c-a19b-e34ecb21b043",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
