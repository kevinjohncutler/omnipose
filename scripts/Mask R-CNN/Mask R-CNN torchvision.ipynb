{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e2ecbb7-adfe-4497-b52f-159cec73392b",
   "metadata": {},
   "source": [
    "# Mask R-CNN\n",
    "The original Mask R-CNN implenetation is too old to run on one system with modern CUDA drivers etc. (even some forks whcih purport to have updated it for Tensorflow 2.x are not working for me), so I opted to use the `torchvision` version. The goal here is to make it as easy as possible for someone to reproduce this work, and having to deal with the nightmare that is out-of-date tensorflow and CUDA is not tenable. \n",
    "\n",
    "This script adapts a couple key functions (like the data loader and remove_overlaps) from the original train_maskrcnn script from cellpose_omni. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dd4c2c-d031-4dff-9c53-a2ad08a2d9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "import time, os, sys\n",
    "from tifffile import imread\n",
    "\n",
    "# os.environ['MKL_DISABLE_FAST_MM'] = '0'\n",
    "os.environ['LRU_CACHE_CAPACITY'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf4629c-bda5-4c88-8c76-d09d62354e03",
   "metadata": {},
   "source": [
    "### Define data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76968d1-2dfa-46ab-823c-348ff879c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "from cellpose_omni import io, transforms\n",
    "from omnipose.utils import format_labels\n",
    "import skimage.io\n",
    "from tifffile import imread\n",
    "import omnipose\n",
    "\n",
    "from pathlib import Path\n",
    "def getname(path,suffix='_masks'):\n",
    "    return os.path.splitext(Path(path).name)[0].replace(suffix,'')\n",
    "\n",
    "class BacteriaDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        mask_filter = '_masks'\n",
    "        img_filter = ''\n",
    "        img_names = io.get_image_files(root,mask_filter,img_filter=img_filter,look_one_level_down=True)\n",
    "        mask_names = io.get_label_files(img_names, mask_filter, img_filter=img_filter)\n",
    "#         self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "#         self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "        self.imgs = sorted(img_names,key=getname)\n",
    "        self.masks = sorted(mask_names,key=getname)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         print(idx)\n",
    "        # load images and masks\n",
    "        img_path = os.path.join(self.root, self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, self.masks[idx])\n",
    "        img = skimage.io.imread(img_path)#.convert('RGB')\n",
    "        img = np.stack([omnipose.utils.normalize99(img)*(2**8-1)]*3,axis=-1).astype(np.uint8)\n",
    "        img = Image.fromarray(img) # must convert to uint8 for pil  \n",
    "        mask = imread(mask_path)\n",
    "        mask = (np.array(mask))\n",
    "        \n",
    "        # instances are encoded as different numbers\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the integer-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            if xmax!=xmin and ymax!=ymin:\n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "            else:\n",
    "                print('uh oh',idx,obj_ids[i])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.bool)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        \n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "#         torch.cuda.empty_cache()\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9825bc09-731c-4e29-80ef-e97fa8466176",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50157ca3-6d18-4a19-aa70-343352e6d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "def get_instance_segmentation_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False,pretrained_backbone=True)\n",
    "    \n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "    model.roi_heads.detections_per_img = 1000\n",
    "    model.roi_heads.nms_thresh = 0.7\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc7d090-40e0-4689-bdf1-77f757e2e21a",
   "metadata": {},
   "source": [
    "### Define training augmentations\n",
    "Notably, only random flipping is implemented here, as I beleive was the case for the original Mask R-CNN tensorflow implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7622d6-267c-46a5-b19e-020a0ce10b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vision/references/detection/ folder needs to be available in the same directory as this notebook\n",
    "\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44418e5-289b-40db-a70e-daee5e8a1f3b",
   "metadata": {},
   "source": [
    "### Initialize training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb08289c-3112-4376-a7eb-85076b919e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#error without lowering this\n",
    "num_workers = 0\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "traindir = '/home/kcutler/DataDrive/omnipose_all/phase/train_sorted'\n",
    "testdir = '/home/kcutler/DataDrive/omnipose_all/phase/test_sorted'\n",
    "# traindir = '/home/kcutler/DataDrive/omnipose_maskrcnn/train'\n",
    "# testdir = '/home/kcutler/DataDrive/omnipose_maskrcnn/test'\n",
    "dataset = BacteriaDataset(traindir, get_transform(train=True))\n",
    "dataset_test = BacteriaDataset(testdir, get_transform(train=False))\n",
    "# split the dataset in train and test set\n",
    "# torch.manual_seed(1)\n",
    "# indices = torch.randperm(len(dataset)).tolist()\n",
    "# dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "# dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=num_workers,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=num_workers,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a890235-c748-49d2-9368-e93c041d2156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to be sure it is correct\n",
    "im,t = dataset[52]\n",
    "im = dataset.__getitem__(2)[0]\n",
    "plt.imshow(im[0])\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbc7dc9-6e0b-4a55-a2cb-f7924e5e6808",
   "metadata": {},
   "source": [
    "### Define model and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b925f55f-ef00-40b8-a093-79d60e2d667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_instance_segmentation_model(num_classes)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec96021-3e7e-4908-817c-ec9d379ade06",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Train\n",
    "Using recommended/default parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bc9415-9afd-4e5d-91f2-dcf68f2372b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean = 0\n",
    "if clean:\n",
    "    num_epochs = 200\n",
    "    for epoch in range(num_epochs):\n",
    "        # train for one epoch, printing every 10 iterations\n",
    "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=20)\n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        # evaluate on the test dataset\n",
    "    #     evaluate(model, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad48db48-030b-4713-98a3-6ec68c9f0c4f",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3e28ad-e9b6-47e9-9436-81774ba176f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldir = '/home/kcutler/DataDrive/maskrcnn/bacterialtrain200epochs_1000detections_per_img_v2'\n",
    "if clean:\n",
    "    torch.save(model, modeldir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4a12e0-4a0a-4f79-a350-b3994fb59736",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate model\n",
    "Test it our on a single image first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c47356-c43e-42fc-9997-ebc94ecb1d5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# device =  torch.device('cpu')\n",
    "model = torch.load(modeldir).to(device)\n",
    "\n",
    "# pick one image from the test set\n",
    "img, _ = dataset_test[0]\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "# model.roi_heads.detections_per_img = 1000 #not needed \n",
    "model.roi_heads.nms_thresh = 1\n",
    "# model.roi_heads.score_thresh\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d17bcb-7004-47c3-aaeb-db3c8766eda5",
   "metadata": {},
   "source": [
    "### Batch process and reconstruct cell masks\n",
    "Mask R-CNN predicts bounding boxes and cell probability within those bounding boxes, along with a confidence score form 0-1. The output is sorted from high to low scores. My first approach was basic approach to loop over all masks and append to a label matrix with incrementing labels. I optimized this a little bit by appending the highest scores last, thereby overwriting low-confidence labels with the higher-confidence ones, but there are a lot of issues with this. After much fiddling, I came up with the following mask reconstruction algorithm: \n",
    "\n",
    "1. Use hysteresis thresholding on cell probability to get a candidate mask\n",
    "2. Check to see if it overlaps with any cells 50% or more and is above a minimum area; if so, just add to that existing mask.\n",
    "3. Otherwise, set its pixels (any that don't overlap with existing masks, which are at higher confidence and should not be overwritten) to a new label value.\n",
    "\n",
    "Comparing the raw summed mask output to my generated masks, I think this gives us pretty much all we can from the really poor Mask R-CNN output. Note that for some of the really large 2kx2k images, the RAM usage is obscene - above 40GB. In a previous iteration, I never noticed this because I was running on a machine with 128GB of RAM. This resource usage comes a little from the neural network prediction on CPU (not enough VRAM to do it on GPU for those images either) but mostly from the intermediate post-processing steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57efbbbd-eeae-432b-ac76-f5e3905705cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device =  torch.device('cpu')\n",
    "model = torch.load(modeldir).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0966604-a13d-4d1c-af91-95900a39cffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastremap \n",
    "\n",
    "savedir = '/home/kcutler/DataDrive/omnipose_all/bact_phase_comparison/maskrcnn'\n",
    "io.check_dir(savedir)\n",
    "\n",
    "nimg = len(dataset_test)\n",
    "final_masks = [[] for i in range(nimg)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468191ef-0b97-4003-8210-84522a30bcbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skimage import filters\n",
    "from cellpose_omni.metrics import _label_overlap\n",
    "\n",
    "min_area = 100\n",
    "\n",
    "# for i in range(nimg):\n",
    "for i in range(56,nimg):\n",
    "\n",
    "# for i in [0]:\n",
    "    entry, path = dataset_test[i], dataset_test.masks[i]\n",
    "    print(getname(path))\n",
    "    img = entry[0]\n",
    "    # put the model in evaluation mode\n",
    "    model.eval()\n",
    "    model.roi_heads.nms_thresh = 1\n",
    "    # model.roi_heads.score_thresh\n",
    "    with torch.no_grad():\n",
    "        prediction = model([img.to(device)])\n",
    "    \n",
    "    labels = np.zeros(img.shape[-2:],dtype=np.uint32)\n",
    "\n",
    "    scores = prediction[0]['scores'].detach().cpu().numpy() # outputs in descending order\n",
    "\n",
    "    cutoff = np.percentile(scores,25)\n",
    "    inds = np.where(scores>cutoff)[0]\n",
    "    l = 1\n",
    "    for j in inds:\n",
    "        pred = np.array((prediction[0]['masks'][j, 0].cpu()))\n",
    "        mask_threshold = .8\n",
    "        m =  filters.apply_hysteresis_threshold(pred, mask_threshold-.25, mask_threshold)\n",
    "        if not np.any(labels):\n",
    "            labels[m] = l\n",
    "        elif np.any(m):\n",
    "            overlap = _label_overlap(labels,np.uint(m))\n",
    "            match = np.argmax(overlap[:,1])\n",
    "            area = np.sum(m)\n",
    "            pix = np.logical_and(m,labels==0) #only add to areas where there are no cell pixels yet\n",
    "            if match==0 or overlap[match,1]/area < 0.5 and area>min_area: \n",
    "                l+=1 # only increment if there is not significant overlap\n",
    "                labels[pix] = l\n",
    "            else:\n",
    "                labels[pix] = match\n",
    "    \n",
    "    # del prediction \n",
    "    labels = fastremap.refit(labels)\n",
    "    \n",
    "    final_masks[i] = labels\n",
    "    io.imsave(os.path.join(savedir,getname(path)+'_masks.tif'),labels)\n",
    "    print(i,'{}% done'.format((i+1)/nimg*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e6eb8d-349a-4d6d-b3d5-848a4eec5b96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ncolor\n",
    "plt.imshow(ncolor.label(labels,max_depth=20),interpolation='None')\n",
    "# plt.imshow(labels)\n",
    "np.max(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e595a7-f981-44a8-9a99-59da5557e66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlapping_masks_test = np.stack([np.array((prediction[0]['masks'][j, 0].cpu())>0.9) for j in inds],axis=0)\n",
    "print(len(overlapping_masks_test),overlapping_masks_test.shape)\n",
    "\n",
    "medians_test = []\n",
    "for mask in overlapping_masks_test:\n",
    "   \n",
    "    ypix, xpix = np.nonzero(mask)\n",
    "    medians_test.append((np.array([ypix.mean(), xpix.mean()])))\n",
    "\n",
    "labels_test = np.int32(remove_overlaps(overlapping_masks_test, overlapping_masks_test.sum(axis=0), np.array(medians_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8112f4ff-db2f-4747-918a-dbbd3712f01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.stack([np.array((prediction[0]['masks'][j, 0].cpu())) for j in inds],axis=0).sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5ae9a0-6104-41ac-818f-5bfc8185b2ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# new algo: loop through all, generate hysteresis mask, compare overlap. If the overlap is high, add to existing label. \n",
    "# If overlap is low, make new label. \n",
    "\n",
    "from skimage import filters\n",
    "from cellpose_omni.metrics import _label_overlap\n",
    "labels = np.zeros(img.shape[-2:],dtype=np.uint32)\n",
    "\n",
    "\n",
    "# for j in range(0,10):\n",
    "# for j in [28]:\n",
    "\n",
    "cutoff = np.percentile(scores,50)\n",
    "inds = np.where(scores>cutoff)[0]\n",
    "min_area = 100\n",
    "l = 1\n",
    "for j in inds:\n",
    "    pred = np.array((prediction[0]['masks'][j, 0].cpu()))\n",
    "    mask_threshold = .8\n",
    "    m =  filters.apply_hysteresis_threshold(pred, mask_threshold-.25, mask_threshold)\n",
    "    if not np.any(labels):\n",
    "        labels[m] = l\n",
    "        print('adding first label')\n",
    "    elif np.any(m):\n",
    "        overlap = _label_overlap(labels,np.uint(m))\n",
    "        match = np.argmax(overlap[:,1])\n",
    "        area = np.sum(m)\n",
    "        pix = np.logical_and(m,labels==0)\n",
    "        if match==0 or overlap[match,1]/area < 0.5 and area>min_area: \n",
    "            l+=1 # only increment if there is not significant overlap\n",
    "            labels[pix] = l\n",
    "        else:\n",
    "            labels[pix] = match\n",
    "    # fig = plt.figure(figsize=(2,2))\n",
    "    # plt.imshow(np.hstack((labels,m,pred)),interpolation='none')\n",
    "    # plt.axis('off')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daf9224-6ec6-4b7b-9ba5-7783071e6051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(np.array((prediction[0]['masks'][1, 0].cpu())))\n",
    "# plt.imshow(img.numpy().transpose(1,2,0))\n",
    "# plt.imshow(ncolor.label(labels,max_depth=20))\n",
    "plt.imshow(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd9a69d-6d88-453d-8c44-1e7454073de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img.numpy().transpose(1,2,0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a61e66-ce72-4c2b-858f-c4b97cf98da4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for mask in [final_masks[i] for i in range(59,100)]:\n",
    "    if mask.ndim==2:\n",
    "        plt.imshow(mask)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aaf9d5-875e-4001-acf6-d71885519081",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_masks[60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b02f4a-fb4b-44ac-925d-5b46ad25a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9837795a-5c28-4c1b-9168-8432379fa46b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
