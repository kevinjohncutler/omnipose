{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e2ecbb7-adfe-4497-b52f-159cec73392b",
   "metadata": {},
   "source": [
    "# Mask R-CNN\n",
    "The original Mask R-CNN implenetation is too old to run on one system with modern CUDA drivers etc. (even some forks whcih purport to have updated it for Tensorflow 2.x are not working for me), so I opted to use the `torchvision` version. The goal here is to make it as easy as possible for someone to reproduce this work, and having to deal with the nightmare that is out-of-date tensorflow and CUDA is not tenable. \n",
    "\n",
    "This script adapts a couple key functions (like the data loader and remove_overlaps) from the original train_maskrcnn script from Cellpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dd4c2c-d031-4dff-9c53-a2ad08a2d9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "import time, os, sys\n",
    "from tifffile import imread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf4629c-bda5-4c88-8c76-d09d62354e03",
   "metadata": {},
   "source": [
    "### Define data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76968d1-2dfa-46ab-823c-348ff879c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "from cellpose import io, transforms\n",
    "from omnipose.utils import format_labels\n",
    "import skimage.io\n",
    "from tifffile import imread\n",
    "import omnipose\n",
    "\n",
    "from pathlib import Path\n",
    "def getname(path,suffix='_masks'):\n",
    "    return os.path.splitext(Path(path).name)[0].replace(suffix,'')\n",
    "\n",
    "class BacteriaDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        mask_filter = '_masks'\n",
    "        img_filter = ''\n",
    "        img_names = io.get_image_files(root,mask_filter,img_filter=img_filter,look_one_level_down=True)\n",
    "        mask_names = io.get_label_files(img_names, mask_filter, img_filter=img_filter)\n",
    "#         self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "#         self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "        self.imgs = sorted(img_names,key=getname)\n",
    "        self.masks = sorted(mask_names,key=getname)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         print(idx)\n",
    "        # load images and masks\n",
    "        img_path = os.path.join(self.root, self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, self.masks[idx])\n",
    "        img = skimage.io.imread(img_path)#.convert('RGB')\n",
    "        img = np.stack([omnipose.utils.normalize99(img)*(2**8-1)]*3,axis=-1).astype(np.uint8)\n",
    "        img = Image.fromarray(img) # must convert to uint8 for pil  \n",
    "        mask = imread(mask_path)\n",
    "        mask = (np.array(mask))\n",
    "        \n",
    "        # instances are encoded as different numbers\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the integer-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            if xmax!=xmin and ymax!=ymin:\n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "            else:\n",
    "                print('uh oh',idx,obj_ids[i])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.bool)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        \n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "#         torch.cuda.empty_cache()\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9825bc09-731c-4e29-80ef-e97fa8466176",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50157ca3-6d18-4a19-aa70-343352e6d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "def get_instance_segmentation_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False,pretrained_backbone=True)\n",
    "    \n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "    model.roi_heads.detections_per_img = 1000\n",
    "    model.roi_heads.nms_thresh = 0.7\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc7d090-40e0-4689-bdf1-77f757e2e21a",
   "metadata": {},
   "source": [
    "### Define training augmentations\n",
    "Notably, only random flipping is implemented here, as I beleive was the case for the original Mask R-CNN tensorflow implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7622d6-267c-46a5-b19e-020a0ce10b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vision/references/detection/ folder needs to be available in the same directory as this notebook\n",
    "\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44418e5-289b-40db-a70e-daee5e8a1f3b",
   "metadata": {},
   "source": [
    "### Initialize training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb08289c-3112-4376-a7eb-85076b919e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#error without lowering this\n",
    "num_workers = 0\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "traindir = '/home/kcutler/DataDrive/omnipose_all/phase/train_sorted'\n",
    "testdir = '/home/kcutler/DataDrive/omnipose_all/phase/test_sorted'\n",
    "# traindir = '/home/kcutler/DataDrive/omnipose_maskrcnn/train'\n",
    "# testdir = '/home/kcutler/DataDrive/omnipose_maskrcnn/test'\n",
    "dataset = BacteriaDataset(traindir, get_transform(train=True))\n",
    "dataset_test = BacteriaDataset(testdir, get_transform(train=False))\n",
    "# split the dataset in train and test set\n",
    "# torch.manual_seed(1)\n",
    "# indices = torch.randperm(len(dataset)).tolist()\n",
    "# dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "# dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=num_workers,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=num_workers,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a890235-c748-49d2-9368-e93c041d2156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to be sure it is correct\n",
    "im,t = dataset[52]\n",
    "im = dataset.__getitem__(2)[0]\n",
    "plt.imshow(im[0])\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbc7dc9-6e0b-4a55-a2cb-f7924e5e6808",
   "metadata": {},
   "source": [
    "### Define model and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b925f55f-ef00-40b8-a093-79d60e2d667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_instance_segmentation_model(num_classes)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec96021-3e7e-4908-817c-ec9d379ade06",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Train\n",
    "Using recommended/default parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bc9415-9afd-4e5d-91f2-dcf68f2372b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean = 0\n",
    "if clean:\n",
    "    num_epochs = 200\n",
    "    for epoch in range(num_epochs):\n",
    "        # train for one epoch, printing every 10 iterations\n",
    "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=20)\n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        # evaluate on the test dataset\n",
    "    #     evaluate(model, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad48db48-030b-4713-98a3-6ec68c9f0c4f",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3e28ad-e9b6-47e9-9436-81774ba176f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldir = '/home/kcutler/DataDrive/maskrcnn/bacterialtrain200epochs_1000detections_per_img_v2'\n",
    "if clean:\n",
    "    torch.save(model, modeldir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4a12e0-4a0a-4f79-a350-b3994fb59736",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate model\n",
    "Test it our on a single image first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9612e43-b882-4f39-aeeb-9fe8d40559cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# device =  torch.device('cpu')\n",
    "model = torch.load(modeldir).to(device)\n",
    "\n",
    "# pick one image from the test set\n",
    "img, _ = dataset_test[0]\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "# model.roi_heads.detections_per_img = 1000 #not needed \n",
    "model.roi_heads.nms_thresh = 1\n",
    "# model.roi_heads.score_thresh\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d17bcb-7004-47c3-aaeb-db3c8766eda5",
   "metadata": {},
   "source": [
    "### Reconstruct cell masks\n",
    "Mask R-CNN predicts bounding boxes and cell probability within those bounding boxes, along with a confidence score form 0-1. The output is sorted from high to low scores. There are two way to use this information to reconstruct labels:\n",
    "1. Loop over all masks and append to a label matrix with incrementing labels. I optimized this a little bit by appending the highest scores last, thereby overwriting low-confidence labels with the higher confidence ones. \n",
    "2. Same as above, but instead of just appending, send any overlapping pixels to the closest mask. The difference here is that proximity determines the overwriting of pixels rather than confidence. \n",
    "\n",
    "You could imagine some more sophisticated permutations on these themes, but as you will see, the output is too poor to be meaningfully rescued. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2236a5-98d9-4514-b566-e6f549949f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = prediction[0]['scores'].cpu().numpy() # outputs in descending order\n",
    "cutoff = np.percentile(scores,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7211b5c7-8bae-47c3-bde6-25f6d2ba69c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method (1)\n",
    "\n",
    "im = img.permute(1, 2, 0).byte().numpy()\n",
    "print(im.shape)\n",
    "r = np.zeros(im.shape[:2])\n",
    "l=0\n",
    "\n",
    "print(scores.shape)\n",
    "inds = np.where(scores>cutoff)[0]\n",
    "for j in np.flip(inds):\n",
    "    l += 1\n",
    "    m = (prediction[0]['masks'][j, 0].cpu())>0.9\n",
    "    r[m] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c1f5f-6230-4433-9d6c-04c74dbff22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method (2)\n",
    "import sys\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "def remove_overlaps(masks, cellpix, medians):\n",
    "    \"\"\" replace overlapping mask pixels with mask id of closest mask\n",
    "        masks = Nmasks x Ly x Lx\n",
    "    \"\"\"\n",
    "    masks=masks.copy()\n",
    "    overlaps = np.array(np.nonzero(cellpix>1.5)).T\n",
    "    dists = ((overlaps[:,:,np.newaxis] - medians.T)**2).sum(axis=1) #square distances \n",
    "    tocell = np.argmin(dists, axis=1) # index of closest mask\n",
    "    masks[:, overlaps[:,0], overlaps[:,1]] = 0 #zero out all overlapping pixels\n",
    "    masks[tocell, overlaps[:,0], overlaps[:,1]] = 1 # put those pixels in\n",
    "\n",
    "    # labels should be 1 to masks.shape[0]\n",
    "    masks = masks.astype(int) * np.arange(1,masks.shape[0]+1,1,int)[:,np.newaxis,np.newaxis]\n",
    "    masks = masks.sum(axis=0)\n",
    "    \n",
    "    # # print out variable size\n",
    "    # for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
    "    #                          key= lambda x: -x[1])[:10]:\n",
    "    #     print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))\n",
    "    \n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b313d5-1bb9-4826-9eb1-42319f7a1638",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the masks is actually a list of nmasks.Ly.Lx\n",
    "# 0.9 is a good cutoff just based on the histogram. Could use something more sophisticated like hysteresis thresholding. \n",
    "overlapping_masks_test = np.stack([np.array((prediction[0]['masks'][j, 0].cpu())>0.9) for j in inds],axis=0)\n",
    "print(len(overlapping_masks_test),overlapping_masks_test.shape)\n",
    "\n",
    "medians_test = []\n",
    "for mask in overlapping_masks_test:\n",
    "   \n",
    "    ypix, xpix = np.nonzero(mask)\n",
    "    medians_test.append((np.array([ypix.mean(), xpix.mean()])))\n",
    "\n",
    "labels_test = np.int32(remove_overlaps(overlapping_masks_test, overlapping_masks_test.sum(axis=0), np.array(medians_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00910781-2696-4fb1-8c42-547fba802839",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cellpix_test = np.stack([np.array((prediction[0]['masks'][j, 0].cpu())>0.9) for j in inds],axis=0).sum(axis=0)\n",
    "cellpix = overlapping_masks.sum(axis=0)\n",
    "np.unique(cellpix_test),np.unique(cellpix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bbe0f6-e74e-4a7c-958e-056c0dc8e0d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.imshow(ncolor.label(cellpix,max_depth=20))\n",
    "plt.imshow(cellpix>1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcce08a-1e46-4dc3-b1be-abd98e0859c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlapping_masks.sum(axis=0).shape\n",
    "import ncolor\n",
    "plt.imshow(np.hstack((ncolor.label(labels_test,max_depth=20),ncolor.label(np.uint32(r)))),interpolation='none')\n",
    "plt.axis('off')\n",
    "# plt.imshow(labels,interpolation='none')\n",
    "print(len(np.unique(labels_test)),len(np.unique(r)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee36f0e9-a1c8-4e18-a0b5-e6dd7decb44a",
   "metadata": {},
   "source": [
    "### Batch process\n",
    "\n",
    "Looks like method (2) is slightly better, so I'll use that for the entire dataset. Ran out of VRAM for larger images so I had to resort to running on CPU. Turns out it was faster than running on GPU? Not sure how... but regardless, both still very slow compared to more modern methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57efbbbd-eeae-432b-ac76-f5e3905705cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device =  torch.device('cpu')\n",
    "model = torch.load(modeldir).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34878ef-8e62-4f31-8bf9-b4e8f227e002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0966604-a13d-4d1c-af91-95900a39cffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastremap \n",
    "\n",
    "savedir = '/home/kcutler/DataDrive/omnipose_all/bact_phase_comparison/maskrcnn'\n",
    "io.check_dir(savedir)\n",
    "\n",
    "nimg = len(dataset_test)\n",
    "final_masks = [[] for i in range(nimg)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180e61fd-908c-4739-9135-095a1904963a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# for i in range(nimg):\n",
    "# for i in range(55,nimg):\n",
    "for i in [0]:\n",
    "    entry, path = dataset_test[i], dataset_test.masks[i]\n",
    "    print(getname(path))\n",
    "    img = entry[0]\n",
    "    # put the model in evaluation mode\n",
    "    model.eval()\n",
    "    model.roi_heads.nms_thresh = 1\n",
    "    # model.roi_heads.score_thresh\n",
    "    with torch.no_grad():\n",
    "        prediction = model([img.to(device)])\n",
    "        \n",
    "    scores = prediction[0]['scores'].detach().cpu().numpy() # outputs in descending order\n",
    "\n",
    "    cutoff = np.percentile(scores,25)\n",
    "    inds = np.where(scores>cutoff)[0]\n",
    "    \n",
    "  \n",
    "    overlapping_masks = [] #[np.zeros(img.shape[-2:],dtype=np.uint8)]\n",
    "    medians = [] #[[np.nan]*2]\n",
    "    drop = []\n",
    "    for j in inds:\n",
    "        pred = np.array((prediction[0]['masks'][j, 0].detach().cpu()))\n",
    "        bin0 = pred>0.9 # somewhat arbitrary cutoff\n",
    "        if np.any(bin0):\n",
    "            overlapping_masks.append(bin0)\n",
    "            pix = np.nonzero(bin0)\n",
    "            medians.append((np.array([p.mean() for p in pix])))\n",
    "        else:\n",
    "            drop.append(j)\n",
    "\n",
    "\n",
    "    if len(overlapping_masks)>0:\n",
    "        overlapping_masks = np.stack(overlapping_masks,axis=0)\n",
    "        del prediction # free up memory, remove_overlaps uses a ridiculous amount for large images, if >system then kernel restarts \n",
    "        labels = np.int32(remove_overlaps(overlapping_masks, overlapping_masks.sum(axis=0), np.array(medians)))\n",
    "        labels = fastremap.refit(labels)\n",
    "    else:\n",
    "        labels = np.zeros(img.shape[-2:],dtype=np.uint8)\n",
    "        print('No masks found.')\n",
    "\n",
    "\n",
    "\n",
    "#     overlapping_masks = np.stack([np.array((prediction[0]['masks'][j, 0].cpu())>0.9) for j in inds],axis=0)\n",
    "#     medians = []\n",
    "#     for mask in overlapping_masks: #runtime warning harmless, but see below\n",
    "#         pix = np.nonzero(mask)\n",
    "#         medians.append((np.array([p.mean() for p in pix])))\n",
    "\n",
    "#     del prediction # free up memory, remove_overlaps uses a ridiculous amount for large images, if >system then kernel restarts \n",
    "#     labels = np.int32(remove_overlaps(overlapping_masks, overlapping_masks.sum(axis=0), np.array(medians)))\n",
    "#     labels = fastremap.refit(labels)\n",
    "    \n",
    "    final_masks[i] = labels\n",
    "    io.imsave(os.path.join(savedir,getname(path)+'_masks.tif'),labels)\n",
    "    print(i,'{}% done'.format((i+1)/nimg*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c2da7a-a645-4da6-915b-d2dff32ca772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(np.hstack((ncolor.label(labels,max_depth=20),ncolor.label(np.uint32(r)))),interpolation='none')\n",
    "# plt.imshow(labels)\n",
    "pred = np.array((prediction[0]['masks'][0, 0].detach().cpu()))\n",
    "bin0 = pred>0.9 # somew\n",
    "# # labels?\n",
    "\n",
    "plt.imshow(bin0)\n",
    "np.any(bin0)\n",
    "# medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652e523c-7c40-42a8-990e-d091add1dfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08193580-c76f-4ef0-8915-3bf65ab42b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(overlapping_masks[0])\n",
    "plt.imshow(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c74c4a5-d2b7-4aa2-8fbf-0853fe32061f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "    # This alternative branch attemps to get rid of all empty masks, but it breaks the mask recontruction, \n",
    "    # not sure yet why \n",
    "    \n",
    "#     overlapping_masks = []\n",
    "#     for j in inds:\n",
    "#         pred = np.array((prediction[0]['masks'][j, 0].detach().cpu()))\n",
    "#         bin0 = pred>0.9 # somewhat arbitrary cutoff\n",
    "#         if np.any(bin0):\n",
    "#             overlapping_masks.append(bin0)\n",
    "    \n",
    "#     if len(overlapping_masks)>0:\n",
    "#         medians = []\n",
    "#         for k,mask in enumerate(overlapping_masks):\n",
    "\n",
    "#             pix = np.nonzero(mask)\n",
    "#             medians.append((np.array([p.mean() for p in pix])))\n",
    "        \n",
    "#         overlapping_masks = np.stack(overlapping_masks,axis=0)\n",
    "\n",
    "#         del prediction # free up memory, remove_overlaps uses a ridiculous amount for large images, if >system then kernel restarts \n",
    "#         labels = np.int32(remove_overlaps(overlapping_masks, overlapping_masks.sum(axis=0), np.array(medians)))\n",
    "#         labels = fastremap.refit(labels)\n",
    "        \n",
    "#     else:\n",
    "#         labels = np.zeros(img.shape[-2:],dtype=np.uint8)\n",
    "#         print('No masks found.')\n",
    "medians_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ebed73-2eb5-4d44-a20f-3e4cc00a2a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(np.array((prediction[0]['masks'][j, 0].cpu()))==np.array((prediction[0]['masks'][j, 0].detach().cpu())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa75626f-a6a9-42f4-a445-fb3473e9c481",
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.all(overlapping_masks_test[k]==overlapping_masks[k]) for k in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dc829e-2cce-4019-8853-f09891a30592",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(20):\n",
    "# plt.imshow(overlapping_masks_test[k]==overlapping_masks[k],interpolation='None')\n",
    "    print(np.any(overlapping_masks_test[k]),np.any(overlapping_masks[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77af7adc-96a1-49ed-858f-b18050e809ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "medians[0]==medians_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a42785-54a3-4202-9f7a-b88690f8d495",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(overlapping_masks.sum(axis=0)==overlapping_masks_test.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc367629-0319-4974-b5d1-d3778bbaaf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ca044f-cc75-4bba-a4ac-a702b85daeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlapping_masks.shape,overlapping_masks_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6792744-5ab8-4257-b2cc-8c99bf10034f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.array(medians).shape, np.array(medians_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88878fca-5910-420a-9177-724995da789e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_overlaps(masks, cellpix, medians):\n",
    "    \"\"\" replace overlapping mask pixels with mask id of closest mask\n",
    "        masks = Nmasks x Ly x Lx\n",
    "    \"\"\"\n",
    "    masks=masks.copy()\n",
    "    overlaps = np.array(np.nonzero(cellpix>1.5)).T\n",
    "    dists = ((overlaps[:,:,np.newaxis] - medians.T)**2).sum(axis=1) #square distances \n",
    "    print(overlaps.shape)\n",
    "    tocell = np.argmin(dists, axis=1) # index of closest mask, BUG with nans, gave 172, any dropped indices work\n",
    "    # print(dists.shape,tocell.shape,dists[0:2,0:1],tocell)\n",
    "    masks[:, overlaps[:,0], overlaps[:,1]] = 0 #zero out all overlapping pixels\n",
    "    masks[tocell, overlaps[:,0], overlaps[:,1]] = 1 # put those pixels in\n",
    "\n",
    "    # labels should be 1 to masks.shape[0]\n",
    "    masks = masks.astype(int) * np.arange(1,masks.shape[0]+1,1,int)[:,np.newaxis,np.newaxis]\n",
    "    print(masks.shape)\n",
    "    masks = masks.sum(axis=0)\n",
    "    \n",
    "#     # print out variable size\n",
    "#     for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
    "#                              key= lambda x: -x[1])[:10]:\n",
    "#         print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))\n",
    "\n",
    "    return masks, dists, tocell\n",
    "\n",
    "m1,d1,t1 = remove_overlaps(overlapping_masks, overlapping_masks.sum(axis=0), np.array(medians))\n",
    "m2,d2,t2= remove_overlaps(overlapping_masks_test, overlapping_masks_test.sum(axis=0), np.array(medians_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c623be-59f2-4ff8-aa8a-0020b376b875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [np.all(d1[:,k]==d2[:,k]) for k in range(20)] # dists the same because medians and cellpix the same\n",
    "# np.any(overlapping_masks_test[-1])\n",
    "len(t1),len(t2),fastremap.unique(t1),fastremap.unique(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b58a32-ff5e-47ff-9a07-d0f76fdb1e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(1,overlapping_masks.shape[0]+1,1,int)[:,np.newaxis,np.newaxis].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9117c4-597b-4bd5-b03e-225cde7b27f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(m1)\n",
    "np.nanargmin(d2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63eda15-448c-4ed2-99fd-f3a3dcf6c34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "medians_test[172]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168c465f-0ba9-402d-84c0-0c4438d50fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d2[:,172]\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b754b-3493-4246-b9ee-7c56afe8478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d401a2d-a01f-4eab-877f-daaee92d0e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e595a7-f981-44a8-9a99-59da5557e66a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
